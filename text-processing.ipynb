{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import re\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report \n",
    "from sklearn.model_selection import GridSearchCV, train_test_split \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "!pip install contractions\n",
    "import contractions \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation orders in California</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed in both directions due to Lake County fire - #CAf...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in the woods...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now in the building across the street</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our area...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT A SECOND I LIVE IN SOUTH TAMPA WHAT AM I GONNA DO ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 18 or 19 days. I've lost count</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage to school bus on 80 in multi car crash #BREAKING</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What's up man?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is ridiculous....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London is cool ;)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love skiing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a wonderful day!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LOOOOOOL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No way...I can't eat that shit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Was in NYC last week!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love my girlfriend</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cooool :)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Do you like pasta?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location  \\\n",
       "0    1     NaN      NaN   \n",
       "1    4     NaN      NaN   \n",
       "2    5     NaN      NaN   \n",
       "3    6     NaN      NaN   \n",
       "4    7     NaN      NaN   \n",
       "5    8     NaN      NaN   \n",
       "6   10     NaN      NaN   \n",
       "7   13     NaN      NaN   \n",
       "8   14     NaN      NaN   \n",
       "9   15     NaN      NaN   \n",
       "10  16     NaN      NaN   \n",
       "11  17     NaN      NaN   \n",
       "12  18     NaN      NaN   \n",
       "13  19     NaN      NaN   \n",
       "14  20     NaN      NaN   \n",
       "15  23     NaN      NaN   \n",
       "16  24     NaN      NaN   \n",
       "17  25     NaN      NaN   \n",
       "18  26     NaN      NaN   \n",
       "19  28     NaN      NaN   \n",
       "20  31     NaN      NaN   \n",
       "21  32     NaN      NaN   \n",
       "22  33     NaN      NaN   \n",
       "23  34     NaN      NaN   \n",
       "24  36     NaN      NaN   \n",
       "25  37     NaN      NaN   \n",
       "26  38     NaN      NaN   \n",
       "27  39     NaN      NaN   \n",
       "28  40     NaN      NaN   \n",
       "29  41     NaN      NaN   \n",
       "\n",
       "                                                                                                   text  \\\n",
       "0                                 Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all   \n",
       "1                                                                Forest fire near La Ronge Sask. Canada   \n",
       "2   All residents asked to 'shelter in place' are being notified by officers. No other evacuation or...   \n",
       "3                                     13,000 people receive #wildfires evacuation orders in California    \n",
       "4              Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school    \n",
       "5   #RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAf...   \n",
       "6       #flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas   \n",
       "7                                           I'm on top of the hill and I can see a fire in the woods...   \n",
       "8                       There's an emergency evacuation happening now in the building across the street   \n",
       "9                                                  I'm afraid that the tornado is coming to our area...   \n",
       "10                                                          Three people died from the heat wave so far   \n",
       "11  Haha South Tampa is getting flooded hah- WAIT A SECOND I LIVE IN SOUTH TAMPA WHAT AM I GONNA DO ...   \n",
       "12                         #raining #flooding #Florida #TampaBay #Tampa 18 or 19 days. I've lost count    \n",
       "13                                                              #Flood in Bago Myanmar #We arrived Bago   \n",
       "14                                             Damage to school bus on 80 in multi car crash #BREAKING    \n",
       "15                                                                                       What's up man?   \n",
       "16                                                                                        I love fruits   \n",
       "17                                                                                     Summer is lovely   \n",
       "18                                                                                    My car is so fast   \n",
       "19                                                                         What a goooooooaaaaaal!!!!!!   \n",
       "20                                                                               this is ridiculous....   \n",
       "21                                                                                    London is cool ;)   \n",
       "22                                                                                          Love skiing   \n",
       "23                                                                                What a wonderful day!   \n",
       "24                                                                                             LOOOOOOL   \n",
       "25                                                                       No way...I can't eat that shit   \n",
       "26                                                                                Was in NYC last week!   \n",
       "27                                                                                   Love my girlfriend   \n",
       "28                                                                                            Cooool :)   \n",
       "29                                                                                   Do you like pasta?   \n",
       "\n",
       "    target  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  \n",
       "10       1  \n",
       "11       1  \n",
       "12       1  \n",
       "13       1  \n",
       "14       1  \n",
       "15       0  \n",
       "16       0  \n",
       "17       0  \n",
       "18       0  \n",
       "19       0  \n",
       "20       0  \n",
       "21       0  \n",
       "22       0  \n",
       "23       0  \n",
       "24       0  \n",
       "25       0  \n",
       "26       0  \n",
       "27       0  \n",
       "28       0  \n",
       "29       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth',100)\n",
    "# Load training set \n",
    "raw_ = pd.read_csv('text-data/train.csv')\n",
    "raw_.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "raw_.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5709, 1) (1904, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5151</th>\n",
       "      <td>@dicehateme @PuppyShogun This makes sense. Paper beats rock paper comes from wood so wood should...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6351</th>\n",
       "      <td>'@CatoInstitute: The causes of federal failure are deeply structural and they will not be easily...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3443</th>\n",
       "      <td>Well as I was chaning an iPad screen it fucking exploded and glass went all over the place. Look...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7164</th>\n",
       "      <td>the war on drugs has turned the U.S. into a WAR zone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7037</th>\n",
       "      <td>Obama Declares Disaster for Typhoon-Devastated Saipan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5159</th>\n",
       "      <td>According to prophecy and also CNN a Mac tablet will completely obliterate the need for other ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>Has body bagged ** RT @d_lac: Drake is body bagging meek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5070</th>\n",
       "      <td>@ConnorFranta #AskConnor if you were a natural disaster what would you be?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>@soapscoop i need you to confirm that ross is dead cause i dont trust anyone else yh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>@libraryeliza he did get a @taylorswift13 'bump' of approval which is probably why he's blown up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3942</th>\n",
       "      <td>Flood Prone Waterways In Westchester County Now Eligible For Millions In State Aid #NewYork - ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>SHOUOUT TO @kasad1lla CAUSE HER VOCALS ARE BLAZING HOT LIKE THE WEATHER SHES IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>driving the avalanche after having my car for a week is like driving a tank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>@aptly_engineerd There is no such curfew.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6657</th>\n",
       "      <td>Three Israeli soldiers wounded in West Bank terrorist attack via /r/worldnews http://t.co/9Tyucd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5517</th>\n",
       "      <td>Reddit's new content policy goes into effect many horrible subreddits banned or quarantined http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2582</th>\n",
       "      <td>Black Eye 9: A space battle occurred at Star O784 involving 3 fleets totaling 3941 ships with 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>@5SOSFamUpdater social casualty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>My hair is poverty at the moment need to get a fade before the weekend gets here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. http://t.co/2nndBGwyEi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     text\n",
       "5151  @dicehateme @PuppyShogun This makes sense. Paper beats rock paper comes from wood so wood should...\n",
       "6351  '@CatoInstitute: The causes of federal failure are deeply structural and they will not be easily...\n",
       "3443  Well as I was chaning an iPad screen it fucking exploded and glass went all over the place. Look...\n",
       "7164                                                the war on drugs has turned the U.S. into a WAR zone.\n",
       "7037                                                Obama Declares Disaster for Typhoon-Devastated Saipan\n",
       "5159  According to prophecy and also CNN a Mac tablet will completely obliterate the need for other ga...\n",
       "1010                                             Has body bagged ** RT @d_lac: Drake is body bagging meek\n",
       "5070                           @ConnorFranta #AskConnor if you were a natural disaster what would you be?\n",
       "2069                 @soapscoop i need you to confirm that ross is dead cause i dont trust anyone else yh\n",
       "931   @libraryeliza he did get a @taylorswift13 'bump' of approval which is probably why he's blown up...\n",
       "3942  Flood Prone Waterways In Westchester County Now Eligible For Millions In State Aid #NewYork - ht...\n",
       "691                       SHOUOUT TO @kasad1lla CAUSE HER VOCALS ARE BLAZING HOT LIKE THE WEATHER SHES IN\n",
       "533                           driving the avalanche after having my car for a week is like driving a tank\n",
       "1928                                                            @aptly_engineerd There is no such curfew.\n",
       "6657  Three Israeli soldiers wounded in West Bank terrorist attack via /r/worldnews http://t.co/9Tyucd...\n",
       "5517  Reddit's new content policy goes into effect many horrible subreddits banned or quarantined http...\n",
       "2582  Black Eye 9: A space battle occurred at Star O784 involving 3 fleets totaling 3941 ships with 13...\n",
       "1450                                                                      @5SOSFamUpdater social casualty\n",
       "652                      My hair is poverty at the moment need to get a fade before the weekend gets here\n",
       "33                     #AFRICANBAZE: Breaking news:Nigeria flag set ablaze in Aba. http://t.co/2nndBGwyEi"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into training and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(raw_[['text']],raw_['target'],random_state=42)\n",
    "print(X_train.shape, X_test.shape)\n",
    "X_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ablaze', 'accident', 'aftershock', 'airplane%20accident',\n",
       "       'ambulance', 'annihilated', 'annihilation', 'apocalypse',\n",
       "       'armageddon', 'army', 'arson', 'arsonist', 'attack', 'attacked',\n",
       "       'avalanche', 'battle', 'bioterror', 'bioterrorism', 'blaze',\n",
       "       'blazing', 'bleeding', 'blew%20up', 'blight', 'blizzard', 'blood',\n",
       "       'bloody', 'blown%20up', 'body%20bag', 'body%20bagging',\n",
       "       'body%20bags'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring what the unique keywords and locations are \n",
    "non_null_kw = raw_.keyword.notnull()\n",
    "non_null_loc = raw_.location.notnull()\n",
    "raw_['keyword'][non_null_kw].unique()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Birmingham', 'Est. September 2012 - Bristol', 'AFRICA',\n",
       "       'Philadelphia, PA', 'London, UK', 'Pretoria', 'World Wide!!',\n",
       "       'Paranaque City', 'Live On Webcam', 'milky way',\n",
       "       'GREENSBORO,NORTH CAROLINA', 'England.',\n",
       "       'Sheffield Township, Ohio', 'India', 'Barbados', 'Anaheim',\n",
       "       'Abuja', 'USA', 'South Africa', 'Sao Paulo, Brazil',\n",
       "       'hollywoodland ', 'Edmonton, Alberta - Treaty 6',\n",
       "       'Inang Pamantasan', 'Twitter Lockout in progress', 'Concord, CA',\n",
       "       'Calgary, AB', 'San Francisco', 'CLVLND', 'Nashville, TN',\n",
       "       'Santa Clara, CA'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_['location'][non_null_loc].unique()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import punkt \n",
    "from nltk import word_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "# create a data cleaning function that tokenizes, \n",
    "# removes english stopwords and punctuations and returns tokenized text in lowercase \n",
    "\n",
    "eng_stop = stopwords.words('english') # english stopwords\n",
    "wn = nltk.WordNetLemmatizer() # Instantiate word lemmatizer\n",
    "\n",
    "def clean_text_lm(text):\n",
    "    '''Removes punctuations and stopwords and returns lowercase tokenized text for input text and pattern'''\n",
    "    # expand contracted sentences\n",
    "    doc = contractions.fix(text) \n",
    "    # match regex pattern and replace with empty string\n",
    "    doc_nospchar = re.sub(r'^a-zA-Z\\s\\W+',r'',doc,re.I | re.A) \n",
    "    # remove punctuations from previous out\n",
    "    doc_nopunct = ''.join([char for char in doc_nospchar if char not in string.punctuation]) \n",
    "    # convert text to lower case and strip white space if any\n",
    "    doc_lower_nospc = doc_nopunct.lower().strip() \n",
    "    # lemmatize and store in list format\n",
    "    lem_text = [wn.lemmatize(word) for word in re.split('\\W+',doc_lower_nospc)] \n",
    "     # join list into string with no stopwords\n",
    "    no_stop_docs = ' '.join([word for word in lem_text if word not in eng_stop])\n",
    "    \n",
    "    return no_stop_docs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5151</th>\n",
       "      <td>@dicehateme @PuppyShogun This makes sense. Paper beats rock paper comes from wood so wood should...</td>\n",
       "      <td>dicehateme puppyshogun make sense paper beat rock paper come wood wood able support obliterate rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6351</th>\n",
       "      <td>'@CatoInstitute: The causes of federal failure are deeply structural and they will not be easily...</td>\n",
       "      <td>catoinstitute cause federal failure deeply structural easily solved httptcoh2xcax4jbu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3443</th>\n",
       "      <td>Well as I was chaning an iPad screen it fucking exploded and glass went all over the place. Look...</td>\n",
       "      <td>well wa chaning ipad screen fucking exploded glass went place look like job going need new one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7164</th>\n",
       "      <td>the war on drugs has turned the U.S. into a WAR zone.</td>\n",
       "      <td>war drug ha turned yous war zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7037</th>\n",
       "      <td>Obama Declares Disaster for Typhoon-Devastated Saipan</td>\n",
       "      <td>obama declares disaster typhoondevastated saipan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5159</th>\n",
       "      <td>According to prophecy and also CNN a Mac tablet will completely obliterate the need for other ga...</td>\n",
       "      <td>according prophecy also cnn mac tablet completely obliterate need gadget combining û httptcoxfcc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>Has body bagged ** RT @d_lac: Drake is body bagging meek</td>\n",
       "      <td>ha body bagged rt dlac drake body bagging meek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5070</th>\n",
       "      <td>@ConnorFranta #AskConnor if you were a natural disaster what would you be?</td>\n",
       "      <td>connorfranta askconnor natural disaster would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>@soapscoop i need you to confirm that ross is dead cause i dont trust anyone else yh</td>\n",
       "      <td>soapscoop need confirm ross dead trust anyone else yh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>@libraryeliza he did get a @taylorswift13 'bump' of approval which is probably why he's blown up...</td>\n",
       "      <td>libraryeliza get taylorswift13 bump approval probably blown httptcokolmzbz1pz musicadvisory</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     text  \\\n",
       "5151  @dicehateme @PuppyShogun This makes sense. Paper beats rock paper comes from wood so wood should...   \n",
       "6351  '@CatoInstitute: The causes of federal failure are deeply structural and they will not be easily...   \n",
       "3443  Well as I was chaning an iPad screen it fucking exploded and glass went all over the place. Look...   \n",
       "7164                                                the war on drugs has turned the U.S. into a WAR zone.   \n",
       "7037                                                Obama Declares Disaster for Typhoon-Devastated Saipan   \n",
       "5159  According to prophecy and also CNN a Mac tablet will completely obliterate the need for other ga...   \n",
       "1010                                             Has body bagged ** RT @d_lac: Drake is body bagging meek   \n",
       "5070                           @ConnorFranta #AskConnor if you were a natural disaster what would you be?   \n",
       "2069                 @soapscoop i need you to confirm that ross is dead cause i dont trust anyone else yh   \n",
       "931   @libraryeliza he did get a @taylorswift13 'bump' of approval which is probably why he's blown up...   \n",
       "\n",
       "                                                                                             cleaned_text  \n",
       "5151  dicehateme puppyshogun make sense paper beat rock paper come wood wood able support obliterate rock  \n",
       "6351                catoinstitute cause federal failure deeply structural easily solved httptcoh2xcax4jbu  \n",
       "3443       well wa chaning ipad screen fucking exploded glass went place look like job going need new one  \n",
       "7164                                                                     war drug ha turned yous war zone  \n",
       "7037                                                     obama declares disaster typhoondevastated saipan  \n",
       "5159  according prophecy also cnn mac tablet completely obliterate need gadget combining û httptcoxfcc...  \n",
       "1010                                                       ha body bagged rt dlac drake body bagging meek  \n",
       "5070                                                        connorfranta askconnor natural disaster would  \n",
       "2069                                                soapscoop need confirm ross dead trust anyone else yh  \n",
       "931           libraryeliza get taylorswift13 bump approval probably blown httptcokolmzbz1pz musicadvisory  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize the function to apply accross dataframe\n",
    "cleaner = np.vectorize(clean_text_lm)\n",
    "# store values in separate column in df\n",
    "X_train['cleaned_text'] = cleaner(X_train[['text']])\n",
    "X_train.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned text is now devoid of any special characters or stopwords, however its still not ready to be vectorized. The first action required is to tokenize the words, ie, converting the sentence into a list of words, and then, there can be many words that have a similar meaning such search, searching, searched, etc. I used a lemmatizer (WordNetLemmatizer) to correlate words with similar meaning and keeps the root words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>So you have a new weapon that can cause un-imaginable destruction.</td>\n",
       "      <td>new weapon unimaginable destruction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just got soaked in a deluge going for pads and tampons...</td>\n",
       "      <td>famping thing gishwhes got soaked deluge going pad tampon thx mishacollins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe CoL police can catch a pickpocket in Liverpool St...</td>\n",
       "      <td>dt georgegalloway rt galloway4mayor ûïthe col police catch pickpocket liverpool stree httptcovxi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Aftershock back to school kick off was great. I want to thank everyone for making it possible. W...</td>\n",
       "      <td>aftershock back school kick wa great want thank everyone making possible great night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>in response to trauma Children of Addicts develop a defensive self - one that decreases vulnerab...</td>\n",
       "      <td>response trauma child addict develop defensive self one decrease vulnerability 3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     text  \\\n",
       "2644                                   So you have a new weapon that can cause un-imaginable destruction.   \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just got soaked in a deluge going for pads and tampons...   \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe CoL police can catch a pickpocket in Liverpool St...   \n",
       "132   Aftershock back to school kick off was great. I want to thank everyone for making it possible. W...   \n",
       "6845  in response to trauma Children of Addicts develop a defensive self - one that decreases vulnerab...   \n",
       "\n",
       "                                                                                             cleaned_text  \n",
       "2644                                                                  new weapon unimaginable destruction  \n",
       "2227                           famping thing gishwhes got soaked deluge going pad tampon thx mishacollins  \n",
       "5448  dt georgegalloway rt galloway4mayor ûïthe col police catch pickpocket liverpool stree httptcovxi...  \n",
       "132                  aftershock back school kick wa great want thank everyone making possible great night  \n",
       "6845                     response trauma child addict develop defensive self one decrease vulnerability 3  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform test set to vectorized format\n",
    "X_test['cleaned_text'] = cleaner(X_test[['text']])\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CountVectorizer\n",
    "CountVec = CountVectorizer(analyzer = 'word',ngram_range = (1,1))\n",
    "# convert text to matrix of token counts\n",
    "train_cmatrix = CountVec.fit_transform(X_train['cleaned_text']).toarray()\n",
    "# convert count matrix to dataframe\n",
    "train_cmatrix_df = pd.DataFrame(count_matrix_arr, columns = CountVec.get_feature_names() )\n",
    "\n",
    "# performing similar steps on test data\n",
    "test_cmatrix = CountVec.transform(X_test['cleaned_text']).toarray()\n",
    "test_cmatrix_df = pd.DataFrame(test_cmatrix,columns = CountVec.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.90      0.83      1091\n",
      "           1       0.83      0.63      0.72       813\n",
      "\n",
      "    accuracy                           0.79      1904\n",
      "   macro avg       0.80      0.77      0.77      1904\n",
      "weighted avg       0.79      0.79      0.78      1904\n",
      "\n",
      "     P    N\n",
      "P  982  109\n",
      "N  297  516\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier\n",
    "rf_clf = RandomForestClassifier(n_jobs=-1)\n",
    "# fit the model \n",
    "rf_model = rf_clf.fit(train_cmatrix, y_train)\n",
    "# predict on test \n",
    "y_hat = rf_model.predict(test_cmatrix)\n",
    "# evaluate model \n",
    "print(classification_report(y_test, y_hat))\n",
    "confusion_ = pd.DataFrame(confusion_matrix(y_test, y_hat),columns=['P','N'],index=['P','N'])\n",
    "print(confusion_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'criterion': 'gini', 'max_depth': 100, 'n_estimators': 150}\n",
      " Best Score: 0.7694873916173327\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.95      0.83      1091\n",
      "           1       0.89      0.55      0.68       813\n",
      "\n",
      "    accuracy                           0.78      1904\n",
      "   macro avg       0.82      0.75      0.76      1904\n",
      "weighted avg       0.80      0.78      0.77      1904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find best params\n",
    "params = {'criterion':['gini','entropy'],'n_estimators':[50,100,150], 'max_depth':[20,50,75,100]}\n",
    "rf = RandomForestClassifier()\n",
    "gs = GridSearchCV(rf ,param_grid = params ,cv = 5, n_jobs=-1)\n",
    "gs.fit(train_cmatrix,y_train)\n",
    "grid_pred = gs.predict(test_cmatrix)\n",
    "best_params = gs.best_params_\n",
    "print(\"Best params: {}\\n Best Score: {}\".format(best_params,gs.best_score_))\n",
    "print(classification_report(y_test,grid_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.95      0.83      1091\n",
      "           1       0.88      0.54      0.67       813\n",
      "\n",
      "    accuracy                           0.77      1904\n",
      "   macro avg       0.81      0.74      0.75      1904\n",
      "weighted avg       0.80      0.77      0.76      1904\n",
      "\n",
      "      P    N\n",
      "P  1031   60\n",
      "N   370  443\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rf_clf = RandomForestClassifier(max_depth=100, n_estimators= 150, n_jobs=-1)\n",
    "# fit the model \n",
    "rf_model = rf_clf.fit(train_cmatrix, y_train)\n",
    "# predict on test \n",
    "y_hat = rf_model.predict(test_cmatrix)\n",
    "# evaluate model \n",
    "print(classification_report(y_test, y_hat))\n",
    "confusion_ = pd.DataFrame(confusion_matrix(y_test, y_hat),columns=['P','N'],index=['P','N'])\n",
    "print(confusion_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using TF-IDF vectorizer and XGB classifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "gb = GradientBoostingClassifier()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
