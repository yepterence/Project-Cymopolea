{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import re\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report \n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, RandomizedSearchCV \n",
    "from nltk.tokenize import punkt  \n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "!pip install contractions\n",
    "import contractions \n",
    "import string\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',100)\n",
    "# Load training set \n",
    "raw_ = pd.read_csv('text-data/train.csv')\n",
    "raw_.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(raw_[['text']],raw_['target'],random_state=42)\n",
    "print(X_train.shape, X_test.shape)\n",
    "X_train[50:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring what the unique keywords and locations are \n",
    "non_null_kw = raw_.keyword.notnull()\n",
    "non_null_loc = raw_.location.notnull()\n",
    "raw_['keyword'][non_null_kw].unique()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_['location'][non_null_loc].unique()[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "# Generate word cloud\n",
    "all_words = ' '.join([text for text in X_train['text']])\n",
    "wordcloud = WordCloud(width=800, height = 500, max_font_size=110).generate(all_words)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing and manipulation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data cleaning function that tokenizes, \n",
    "# removes english stopwords and punctuations and returns tokenized text in lowercase \n",
    "\n",
    "def clean_text_lm(text):\n",
    "    '''Removes punctuations and stopwords and returns lowercase tokenized text for input text and pattern'''\n",
    "    \n",
    "    # expand contracted sentences\n",
    "    doc = contractions.fix(text) \n",
    "    eng_stop = stopwords.words('english') # english stopwords\n",
    "    wn = nltk.WordNetLemmatizer() # Instantiate word lemmatizer\n",
    "    \n",
    "    # match regex pattern and remove users, replace with empty string\n",
    "    #  doc_nousr = re.sub(r'@[^\\s]+',r'',doc)\n",
    "    \n",
    "    # remove any ascii symbols \n",
    "    doc_noascii= doc.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    # remove any links\n",
    "    doc_nourl = re.sub(r'(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-&?=%.]+',r'',doc_noascii)\n",
    "    # remove any remaining special characters\n",
    "    doc_nospchar = re.sub(r'^a-zA-Z\\s\\W+',r'',doc_nourl,re.I | re.A) \n",
    "    \n",
    "    # remove punctuations from previous out\n",
    "    doc_nopunct = ''.join([char for char in doc_nospchar if char not in string.punctuation])\n",
    "    \n",
    "    # convert text to lower case and strip white space if any\n",
    "    doc_lower_nospc = doc_nopunct.lower().strip() \n",
    "    \n",
    "    # lemmatize and store in list format\n",
    "    lem_text = [wn.lemmatize(word) for word in re.split('\\W+',doc_lower_nospc)] \n",
    "    #  removes any nonsensical words\n",
    "    lem_text = [word for word in lem_text if len(word) > 2]\n",
    "    \n",
    "     # join list into string with no stopwords\n",
    "    no_stop_docs = ' '.join([word for word in lem_text if word not in eng_stop])\n",
    "    \n",
    "    return no_stop_docs\n",
    "\n",
    "# Vectorize the function to apply accross dataframe\n",
    "cleaner = np.vectorize(clean_text_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store clean text in separate column in df\n",
    "X_train['cleaned_text'] = cleaner(X_train[['text']])\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up text in test set\n",
    "X_test['cleaned_text'] = cleaner(X_test[['text']])\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_['cleaned_text'] = cleaner(raw_['text'])\n",
    "disaster_words = ' '.join(word for word in raw_['cleaned_text'][raw_['target']==1])\n",
    "wordcloud = WordCloud(width=850, height=500, max_font_size=110).generate(disaster_words)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "wordcloud.to_file('images/disaster_wordcloud.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This word cloud shows us word frequency in tweets that are associated to disaster, the larger the text would indicate a higher frequency of the word being used.  \n",
    "The cleaned text is now devoid of any special characters or stopwords, however its still not ready to be vectorized. The first action required is to tokenize the words, ie, converting the sentence into a list of words, and then, there can be many words that have a similar meaning such search, searching, searched, etc. I used a lemmatizer (WordNetLemmatizer) to correlate words with similar meaning and keeps the root words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering\n",
    "As we can see from prior modeling, the accuracy seems to be stagnant at ~77% even though the parameters were tuned using cross validation. Creating new features from the dataset might help with this issue. \n",
    "\n",
    "Utilized the guide on [AnalyticsVidhya](https://www.analyticsvidhya.com/blog/2021/04/a-guide-to-feature-engineering-in-nlp/) as a reference for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Length of text \n",
    "raw_['doc_len'] = raw_['text'].apply(len)\n",
    "raw_['word_count'] = raw_['text'].apply(lambda x: len(x.split()))\n",
    "# Number of caps per tweet \n",
    "raw_['CAPS_len'] = raw_['text'].apply(lambda x: \n",
    "                                   len([word for word in x.split() if word.isupper()])\n",
    "                                  )\n",
    "raw_['sent_len'] = raw_['text'].apply(lambda x: len(nltk.sent_tokenize(x)))\n",
    "raw_['hashtag_count'] = raw_['text'].apply(lambda x: len(re.findall(r'(\\#[A-Za-z0-9]*)',x)))\n",
    "raw_['unique_word_count'] = raw_['text'].apply(lambda x: len(set(x.split())))\n",
    "\n",
    "raw_.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(0, 100, 30)\n",
    "\n",
    "# create subplots, figsize to control size of the image\n",
    "fig, ax = plt.subplots(1,3, figsize=(10,5)) \n",
    "# plt.subplot(1,3,1) # 1 line, 2 rows, index nr 1 (first position in the subplot)\n",
    "ax[0].hist(raw_[raw_['target']==1]['doc_len'], bins, alpha=0.5, label='disaster')\n",
    "ax[0].hist(raw_[raw_['target']==0]['doc_len'], bins, alpha=0.5, label='non-disaster')\n",
    "ax[0].legend(loc='best')\n",
    "# plt.subplot(1, 3, 2) \n",
    "ax[1].hist(raw_[raw_['target']==1]['word_count'], bins, alpha=0.5, label='disaster')\n",
    "ax[1].hist(raw_[raw_['target']==0]['word_count'], bins, alpha=0.5, label='non-disaster')\n",
    "ax[1].legend(loc='best')\n",
    "# plt.subplot(1,3,3)\n",
    "ax[2].hist(raw_[raw_['target']==1]['unique_word_count'], bins, alpha=0.5, label='disaster')\n",
    "ax[2].hist(raw_[raw_['target']==0]['unique_word_count'], bins, alpha=0.5, label='non-disaster')\n",
    "ax[2].legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_bins = np.linspace(0, 21, 3)\n",
    "fig, ax = plt.subplots(1,3, figsize=(10,5)) \n",
    "# plt.subplot(1,3,1) # 1 line, 2 rows, index nr 1 (first position in the subplot)\n",
    "ax[0].hist(raw_[raw_['target']==1]['CAPS_len'], small_bins, alpha=0.5, label='disaster')\n",
    "ax[0].hist(raw_[raw_['target']==0]['CAPS_len'], small_bins, alpha=0.5, label='non-disaster')\n",
    "ax[0].legend(loc='best')\n",
    "# plt.subplot(1, 3, 2) \n",
    "ax[1].hist(raw_[raw_['target']==1]['hashtag_count'], small_bins, alpha=0.5, label='disaster')\n",
    "ax[1].hist(raw_[raw_['target']==0]['hashtag_count'], small_bins, alpha=0.5, label='non-disaster')\n",
    "ax[1].legend(loc='best')\n",
    "# plt.subplot(1,3,3)\n",
    "ax[2].hist(raw_[raw_['target']==1]['sent_len'], small_bins, alpha=0.5, label='disaster')\n",
    "ax[2].hist(raw_[raw_['target']==0]['sent_len'], small_bins, alpha=0.5, label='non-disaster')\n",
    "ax[2].legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with disasters, we can try and derive the sentiment of each tweet based on the commonly used words that are associated to disasters such as death, hurricane, flood etc. \n",
    "NLTK conveniently has a library that can allow us to perform this kind of analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "nltk.download('vader_lexicon')\n",
    "sent_analyze = SentimentIntensityAnalyzer()\n",
    "sample = X_train[:100]\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polar_ = np.vectorize(sent_analyze.polarity_scores)\n",
    "\n",
    "sample['polarity'] = polar_(sample['cleaned_text'])\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the output generated though, its generating negative values and compound values somewhat closely to what we'd like it to be, for eg. entry 7037: we can see negative polarity is ~0.506, compound ~-0.62, however if we observe the entry above \"war on drus..\" has a higher negative polarity rating, which may make it not as useful for our use case. But its worth a try. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CountVectorizer, bag of words model\n",
    "CountVec = CountVectorizer(analyzer = 'word',ngram_range = (1,1))\n",
    "# fit and transform using bag of words model\n",
    "train_cmatrix = CountVec.fit_transform(X_train['cleaned_text']).toarray()\n",
    "# convert count matrix to dataframe\n",
    "train_cmatrix_df = pd.DataFrame(train_cmatrix, columns = CountVec.get_feature_names() )\n",
    "\n",
    "# performing similar steps on test data\n",
    "test_cmatrix = CountVec.transform(X_test['cleaned_text']).toarray()\n",
    "test_cmatrix_df = pd.DataFrame(test_cmatrix,columns = CountVec.get_feature_names())\n",
    "train_cmatrix_df.shape, test_cmatrix_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# range of uni- and bi-gram vectors \n",
    "uni_bigram_vec = CountVectorizer(analyzer = 'word',ngram_range = (1,2))\n",
    "unibigram_train_cmatrix = uni_bigram_vec.fit_transform(X_train['cleaned_text']).toarray()\n",
    "unibigram_test_cmatrix = uni_bigram_vec.transform(X_test['cleaned_text']).toarray()\n",
    "unibigram_train_cmatrix.shape, unibigram_test_cmatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_trigram_vec = CountVectorizer(analyzer = 'word',ngram_range = (2,3))\n",
    "bi_trigram_train_cmatrix = bi_trigram_vec.fit_transform(X_train['cleaned_text']).toarray()\n",
    "bi_trigram_test_cmatrix = bi_trigram_vec.transform(X_test['cleaned_text']).toarray()\n",
    "bi_trigram_train_cmatrix.shape, bi_trigram_test_cmatrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification of tweets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate classifier\n",
    "rf_clf = RandomForestClassifier(random_state=42,n_jobs=-1)\n",
    "# fit the model \n",
    "rf_model = rf_clf.fit(train_cmatrix, y_train)\n",
    "# predict on test \n",
    "y_hat = rf_model.predict(test_cmatrix)\n",
    "\n",
    "# evaluate model \n",
    "print(classification_report(y_test, y_hat))\n",
    "confusion_ = pd.DataFrame(confusion_matrix(y_test, y_hat),index=['non-disaster','disaster'])\n",
    "print(confusion_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning \n",
    "While the performance using default parameters of the classifier yielded decent scoring in precision, recall and f1-score, we should try to see what would be configuration would provide the best score possible using the RandomForest classifier. To do this, we will use grid search cross validation to determine these parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best params using GridSearchCV\n",
    "params = {'criterion':['gini','entropy'],\n",
    "          'n_estimators':[50,100,150], \n",
    "          'max_depth':[50,75,100,None]\n",
    "         }\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf_gs = GridSearchCV(rf ,param_grid = params ,cv = 5, scoring='accuracy', n_jobs=-1)\n",
    "rf_gs.fit(train_cmatrix,y_train)\n",
    "rf_grid_pred = rf_gs.predict(test_cmatrix)\n",
    "best_params = rf_gs.best_params_\n",
    "print(\"Best params: {}\\n Best Score: {}\".format(best_params,rf_gs.best_score_))\n",
    "print(classification_report(y_test,rf_grid_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like default parameters are best for the classification using RandomForest. Trying out other classifiers and vectorizing methods to see if we can get better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "steps = [('Cvect',CountVec),\n",
    "         ('rf',rf)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# cross val > final pipeline\n",
    "\n",
    "rf_param_grid = {'criterion':['gini','entropy'],\n",
    "          'n_estimators':np.arange(50, 250, 50), \n",
    "          'max_depth':np.arange(20, 500, 25)\n",
    "         }\n",
    "# Find best params using RandomizedgridsearchCV\n",
    "rf_randomizedcv_roc_auc = RandomizedSearchCV(rf, \n",
    "                                        param_distributions=rf_param_grid,\n",
    "                                        n_iter=1, \n",
    "                                        scoring=\"roc_auc\", \n",
    "                                        verbose=1, \n",
    "                                        cv=5,\n",
    "                                         n_jobs=-1)\n",
    "rf_randomizedcv_roc_auc.fit(train_cmatrix,y_train)\n",
    "print(rf_randomizedcv_roc_auc.best_score_)\n",
    "print(rf_randomizedcv_roc_auc.best_estimator_)\n",
    "best_rf_clf = rf_randomizedcv_roc_auc.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = best_rf_clf.transform(test_cmatrix)\n",
    "print(classification_report(y_test, y_hat))\n",
    "confusion_ = pd.DataFrame(confusion_matrix(y_test, y_hat),index=['non-disaster','disaster'])\n",
    "sns.heatmap(confusion_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun RF classifier with optimized parameters\n",
    "rf_clf = RandomForestClassifier(max_depth = 70,random_state=42,verbose=1,n_jobs = -1)\n",
    "\n",
    "# fit the model \n",
    "rf_model = rf_clf.fit(train_cmatrix, y_train)\n",
    "# predict on test \n",
    "y_hat = rf_model.predict(test_cmatrix)\n",
    "# evaluate model \n",
    "print(classification_report(y_test, y_hat))\n",
    "confusion_ = pd.DataFrame(confusion_matrix(y_test, y_hat),index=['non-disaster','disaster'])\n",
    "print(confusion_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni-bigram model \n",
    "rf_model_unibigram = rf_clf.fit(unibigram_train_cmatrix,y_train)\n",
    "unibigram_y_hat = rf_model_unibigram.predict(unibigram_test_cmatrix)\n",
    "print(classification_report(y_test, unibigram_y_hat))\n",
    "confusion_ = pd.DataFrame(confusion_matrix(y_test, unibigram_y_hat),index=['non-disaster','disaster'])\n",
    "print(confusion_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bi-trigram model \n",
    "rf_model_bitrigram = rf_clf.fit(bi_trigram_train_cmatrix,y_train)\n",
    "bi_trigram_y_hat = rf_model_unibigram.predict(bi_trigram_test_cmatrix)\n",
    "print(classification_report(y_test, bi_trigram_y_hat))\n",
    "confusion_ = pd.DataFrame(confusion_matrix(y_test, bi_trigram_y_hat),index=['non-disaster','disaster'])\n",
    "print(confusion_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the y_score\n",
    "rf_y_score = rf_model_unibigram.predict_proba(test_cmatrix)\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "#Binarize the output\n",
    "rf_y_test_bin = label_binarize(y_test, classes=[0,1])\n",
    "n_classes = rf_y_test_bin.shape[1]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "#create ROC curve\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(rf_y_test_bin[:, i], rf_y_score[:, i])\n",
    "    plt.plot(fpr[i], tpr[i], color='darkorange', lw=2)\n",
    "    print('AUC for Class {}: {}'.format(i+1, auc(fpr[i], tpr[i])))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='maroon', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Curves')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Drew inspiration for code from [laurenlizz22](https://laurenliz22.github.io/roc_curve_multiclass_predictions_random_forest_classifier) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of results\n",
    "Our optimized model was able to distinguish disaster from non-disaster tweets with ~<b>77%</b> accuracy.\n",
    "<br>\n",
    "From the confusion matrix, there seems to be a marked difference in the classification of true negatives and positives. However, we also see any improvements in the precision, recall or f1-scores when compared to the default model. We also notice a drastic difference in recall scores between the 0 (non-disaster) and 1 (disaster) labels. One reason might because of the class imbalance in the training dataset. \n",
    "<br>\n",
    "It might be worthwhile to look into other classifiers as well as adding features to improve our classification. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using TF-IDF vectorizer and XGB classifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "tf_vect = TfidfVectorizer(analyzer=clean_text_lm, min_df=2, max_df=0.9)\n",
    "train_tfidf_matrix = tf_vect.fit_transform(X_train['cleaned_text']).toarray()\n",
    "test_tfidf_matrix = tf_vect.transform(X_test['cleaned_text']).toarray()\n",
    "gb.fit(train_tfidf_matrix,y_train)\n",
    "gb_y_hat = gb.predict(test_tfidf_matrix)\n",
    "print(classification_report(y_test, gb_y_hat))\n",
    "confusion_ = pd.DataFrame(confusion_matrix(y_test, gb_y_hat),index=['non-disaster','disaster'])\n",
    "print(confusion_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_param_grid = {\n",
    "    'learning_rate': np.arange(0.05, 1, 0.05),\n",
    "    'max_depth': np.arange(3, 10, 1),\n",
    "    'n_estimators': np.arange(50, 200, 50)\n",
    "}\n",
    "# gb_steps=[('tf',tf_vect),('XGB',gb)]\n",
    "# gb_pipeline = Pipeline(steps=gb_steps)\n",
    "# gb_pipeline.fit(train_tfidf_matrix,y_train)\n",
    "gb_randomizedcv_roc_auc = RandomizedSearchCV(gb, \n",
    "                                        param_distributions=gbm_param_grid,\n",
    "                                        n_iter=1, \n",
    "                                        scoring=\"roc_auc\", \n",
    "                                        verbose=1, \n",
    "                                        cv=5,\n",
    "                                         n_jobs=-1)\n",
    "gb_randomizedcv_roc_auc.fit(train_tfidf_matrix,y_train)\n",
    "grid_pred_gbm = gb_randomizedcv_roc_auc.predict(test_tfidf_matrix)\n",
    "best_params_gbm = gb_randomizedcv_roc_auc.best_params_\n",
    "print(\"Best params: {}\\n Best Score: {}\".format(best_params_gbm,randomizedcv_roc_auc.best_score_))\n",
    "print(classification_report(y_test,grid_pred_gbm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
